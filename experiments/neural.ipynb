{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import faiss\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from tqdm.auto import tqdm\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.cluster import KMeans\n",
                "from sentence_transformers import SentenceTransformer"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Semantic Search"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the datasets\n",
                "brand_category_df = pd.read_csv('../data/raw/brand_category.csv')\n",
                "offer_retailer_df = pd.read_csv('../data/raw/offer_retailer.csv')\n",
                "categories_df = pd.read_csv('../data/raw/categories.csv')\n",
                "\n",
                "# Group brand_category_df by 'BRAND_BELONGS_TO_CATEGORY' (brand) and aggregate categories\n",
                "brand_categories = brand_category_df.groupby('BRAND')['BRAND_BELONGS_TO_CATEGORY'].agg(list).reset_index()\n",
                "brand_categories.rename(columns={'BRAND_BELONGS_TO_CATEGORY': 'CATEGORIES'}, inplace=True)\n",
                "\n",
                "# Map the aggregated categories to the 'BRAND' column in offer_retailer_df\n",
                "merged_df = offer_retailer_df.merge(brand_categories, on='BRAND', how='left')\n",
                "\n",
                "# Create a mapping of 'PRODUCT_CATEGORY' to 'IS_CHILD_CATEGORY_TO'\n",
                "category_mapping = categories_df.set_index('PRODUCT_CATEGORY')['IS_CHILD_CATEGORY_TO'].to_dict()\n",
                "\n",
                "# Function to get unique 'IS_CHILD_CATEGORY_TO' values for each brand\n",
                "def get_super_categories(categories):\n",
                "    super_categories = set()\n",
                "    if isinstance(categories, list):\n",
                "        for category in categories:\n",
                "            super_category = category_mapping.get(category)\n",
                "            if super_category:\n",
                "                super_categories.add(super_category)\n",
                "    return list(super_categories) if super_categories else ''\n",
                "\n",
                "# Apply the function to the 'CATEGORIES' column\n",
                "merged_df['SUPER_CATEGORIES'] = merged_df['CATEGORIES'].apply(get_super_categories)\n",
                "\n",
                "# # Preprocess the text data and replace NaN values with an empty string\n",
                "merged_df = merged_df.fillna('')\n",
                "merged_df['TEXT'] = merged_df['TEXT'] = (merged_df['BRAND'] + ' ; ' + merged_df['RETAILER'] + ' ; ' + merged_df['CATEGORIES'].str.join(', ') + ' ; ' + merged_df['SUPER_CATEGORIES'].str.join(', ')).str.lower()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "merged_df.TEXT[1]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "models = ['BAAI/bge-base-en-v1.5', 'all-distilroberta-v1', 'thenlper/gte-base', ]\n",
                "model = SentenceTransformer(models[1])\n",
                "model.to(DEVICE)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### EXP"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import requests\n",
                "\n",
                "API_URL = \"https://api-inference.huggingface.co/models/BAAI/bge-base-en-v1.5\"\n",
                "headers = {\"Authorization\": \"Bearer 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\"}\n",
                "\n",
                "def query(text):\n",
                "\tpayload = {\"inputs\": text}\n",
                "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
                "\treturn response.json()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "emb = model.encode(merged_df.TEXT[1])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "emb_q = query(merged_df.TEXT[1])\n",
                "emb_q = np.array([emb_q]).astype(\"float32\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "emb_a = np.array([emb]).astype(\"float32\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### FAI"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text_embeddings = model.encode(merged_df['TEXT'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "res = faiss.StandardGpuResources()\n",
                "index = faiss.IndexFlatL2(text_embeddings.shape[1])\n",
                "index.metric_type = faiss.METRIC_INNER_PRODUCT\n",
                "# index = faiss.IndexIDMap(index)\n",
                "\n",
                "# gpu_index_flat = faiss.index_cpu_to_gpu(res, 0, index)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# index.add_with_ids(text_embeddings, merged_df.index.values)\n",
                "index.add(text_embeddings)\n",
                "print(index.is_trained, index.ntotal)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "faiss.write_index(index, './stores/bge_embedding.index')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "query_vector = model.encode(\"Walmart\".lower())\n",
                "D, I = index.search(query_vector.reshape(1, -1), 30)\n",
                "print(list(zip(D,I)))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function to search for offers based on user input with a similarity threshold\n",
                "def search_offers(user_input, threshold=0.05, dis_threshold=0.3, k=100):\n",
                "    user_input = user_input.lower()\n",
                "\n",
                "    # Calculate the TF-IDF vectors for user input\n",
                "    user_vector = model.encode(user_input).reshape(1, -1)\n",
                "\n",
                "    # Calculate similarity between user input and each offer\n",
                "    scores, indices = index.search(user_vector.reshape(1, -1), k)\n",
                "\n",
                "    # Sort offers by similarity and return the results above the threshold\n",
                "    results = merged_df.loc[indices[0]]\n",
                "    results['SIMILARITY_SCORE'] = scores[0]\n",
                "    results = results[results['SIMILARITY_SCORE'] > threshold]\n",
                "    # results = results.sort_values(by='SIMILARITY_SCORE', ascending=False)\n",
                "\n",
                "    # Display a bar graph of similarity scores\n",
                "    plt.scatter(results['SIMILARITY_SCORE'], results['SIMILARITY_SCORE'])\n",
                "    plt.xlabel('')\n",
                "    plt.ylabel('')\n",
                "    plt.title('Similarity Scores Scatter Plot')\n",
                "\n",
                "    # Apply K-Means clustering to the similarity scores\n",
                "    n_clusters = 2\n",
                "    kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init='auto')\n",
                "    results['Cluster'] = kmeans.fit_predict(results[['SIMILARITY_SCORE']])\n",
                "    cluster_centers = kmeans.cluster_centers_\n",
                "\n",
                "    # Overlay clusters on the scatter plot\n",
                "    for cluster in range(n_clusters):\n",
                "        cluster_data = results[results['Cluster'] == cluster]\n",
                "        plt.scatter(cluster_data['SIMILARITY_SCORE'], cluster_data['SIMILARITY_SCORE'], label=f'Cluster {cluster}')\n",
                "\n",
                "    # Calculate distances between cluster centers\n",
                "    cluster_0_center = cluster_centers[0]\n",
                "    cluster_1_center = cluster_centers[1]\n",
                "\n",
                "    # Find the lowest point in the cluster with the higher center\n",
                "    lowest_point_cluster_higher_center = results[results['Cluster'] == np.argmax(cluster_centers)]['SIMILARITY_SCORE'].min()\n",
                "\n",
                "    # Find the highest point in the other cluster\n",
                "    highest_point_other_cluster = results[results['Cluster'] != np.argmax(cluster_centers)]['SIMILARITY_SCORE'].max()\n",
                "\n",
                "    # Calculate the Euclidean distance\n",
                "    distance = np.abs(lowest_point_cluster_higher_center - highest_point_other_cluster)\n",
                "    distance_between_centers = np.linalg.norm(cluster_0_center - cluster_1_center)\n",
                "    print(distance, distance_between_centers)\n",
                "\n",
                "    plt.legend()\n",
                "    plt.show()\n",
                "\n",
                "    return results[['OFFER', 'RETAILER', 'BRAND', 'CATEGORIES', 'SIMILARITY_SCORE', 'Cluster']]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example usage:\n",
                "# user_input = input(\"Enter your search query: \")\n",
                "user_input = \"walmartz\"\n",
                "search_results = search_offers(user_input)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "fetch",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
